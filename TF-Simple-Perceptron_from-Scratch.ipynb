{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e3ba44-3ecd-4809-9fa7-950174a39709",
   "metadata": {},
   "source": [
    "\r\n",
    "### Network Architecture Overview\r\n",
    "\r\n",
    "- **Input Layer**: \r\n",
    "  - **Number of Neurons**: 3 (since `x_input` has 3 features: `[1.0, 2.0, 0.0]`)\r\n",
    "  - Each neuron in the input layer represents one feature of the input data.\r\n",
    "\r\n",
    "- **Hidden Layer**:\r\n",
    "  - **Number of Hidden Layers**: 1 (as your architecture currently shows only one layer between the input and output layers).\r\n",
    "  - **Number of Neurons**: 1 (since your weight matrix `w1` has a shape of (3, 1), indicating that it connects 3 input features to one neuron in the hidden layer).\r\n",
    "  - The activation function used is the **sigmoid** function. This is applied to the output of the hidden layer neuron, introducing non-linearity to the model.\r\n",
    "\r\n",
    "- **Output Layer**:\r\n",
    "  - **Number of Neurons**: 1 (your weight matrix `w2` has a shape of (1, 1), indicating the output layer has only one neuron).\r\n",
    "  - This neuron represents the final output of the network, which predicts the target value based on the inputs.\r\n",
    "\r\n",
    "### Visual Representation of the Architecture\r\n",
    "\r\n",
    "Hereâ€™s a simplified visual representation of the architecture:\r\n",
    "\r\n",
    "```\r\n",
    "Input Layer                  Hidden Layer                Output Layer\r\n",
    "   (x1)                         (h1)                            (y)\r\n",
    "     |                            |                              |\r\n",
    "   x[0]  --------------------->  w1                             w2\r\n",
    "   x[1]  --------------------->  sigmoid(h1) ----------------> y_pred\r\n",
    "   x[2]  --------------------->  b1\r\n",
    "```\r\n",
    "\r\n",
    "### Summary of Layers and Neurons\r\n",
    "\r\n",
    "- **Input Layer**: 3 neurons (for 3 input features)\r\n",
    "- **Hidden Layer**: \r\n",
    "  - 1 hidden layer\r\n",
    "  - 1 need more details or if you have further questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a950b6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "499a5e83-0c24-4fa6-ab62-1427711c8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "\n",
    "# Input data  \n",
    "x_input = tf.constant([[1.0, 2.0, 0.0]], dtype=tf.float32)  \n",
    "y_true = tf.constant([[1.0]], dtype=tf.float32)  \n",
    "\n",
    "# Initialize the Weights  \n",
    "w1 = tf.Variable([[0.5], [-0.4], [0.0]], dtype=tf.float32)  # Shape (3, 1) since x has 3 features  \n",
    "b1 = tf.Variable([0.1], dtype=tf.float32)  \n",
    "\n",
    "w2 = tf.Variable([[0.7]], dtype=tf.float32)                 # Shape (1, 1) for hidden layer to output  \n",
    "b2 = tf.Variable([0.0], dtype=tf.float32)  \n",
    "\n",
    "# Optimizer  \n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)  # Ensure SGD is imported from tf.keras.optimizers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3ecc1d-b98b-4dc1-ac1a-543569782953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        #Forward Pass\n",
    "        z1 = tf.matmul(x_input, w1) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "\n",
    "        z2 = tf.matmul(a1, w2) + b2\n",
    "        y_pred = sigmoid(z2)\n",
    "\n",
    "        #Loss Calculation\n",
    "        loss = mse_loss(y_true, y_pred)\n",
    "        \n",
    "    #Backward Pass\n",
    "    grads = tape.gradient(loss,[w1,b1,w2,b2])\n",
    "    optimizer.apply_gradients(zip(grads, [w1,b1,w2,b2]))\n",
    "\n",
    "    return loss.numpy(), y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddad967c-ef9c-42ca-942a-6866f65414d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 0.1716, Prediction: 0.5857\n",
      "Epoch 02, Loss: 0.1705, Prediction: 0.5871\n",
      "Epoch 03, Loss: 0.1694, Prediction: 0.5884\n",
      "Epoch 04, Loss: 0.1683, Prediction: 0.5898\n",
      "Epoch 05, Loss: 0.1672, Prediction: 0.5911\n",
      "Epoch 06, Loss: 0.1661, Prediction: 0.5924\n",
      "Epoch 07, Loss: 0.1650, Prediction: 0.5938\n",
      "Epoch 08, Loss: 0.1639, Prediction: 0.5951\n",
      "Epoch 09, Loss: 0.1629, Prediction: 0.5964\n",
      "Epoch 10, Loss: 0.1618, Prediction: 0.5978\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    loss, y_pred = train_step()\n",
    "    print(f\"Epoch {epoch+1:02d}, Loss: {loss:.4f}, Prediction: {y_pred[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40ac75c4-f572-44df-ac04-33935380088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_input = tf.constant([[1.0]], dtype=tf.float32)\n",
    "y_true = tf.constant([[0.5]], dtype=tf.float32)\n",
    "\n",
    "w = tf.Variable([[0.2]],dtype=tf.float32)\n",
    "b = tf.Variable([0.1], dtype=tf.float32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "def model(x):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        ## Forward Pass\n",
    "        y_pred = model(x_input)\n",
    "    \n",
    "        ## Calculate the loss\n",
    "        loss = tf.reduce_mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    grads = tape.gradient(loss,[w,b])\n",
    "    optimizer.apply_gradients(zip(grads, [w,b]))\n",
    "    return loss.numpy(), y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69a6c8ec-0029-42b9-8e03-740a2f1fce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss : 0.0000, Prediction: 0.4976\n",
      "Epoch : 2, Loss : 0.0000, Prediction: 0.4978\n",
      "Epoch : 3, Loss : 0.0000, Prediction: 0.4980\n",
      "Epoch : 4, Loss : 0.0000, Prediction: 0.4981\n",
      "Epoch : 5, Loss : 0.0000, Prediction: 0.4983\n",
      "Epoch : 6, Loss : 0.0000, Prediction: 0.4984\n",
      "Epoch : 7, Loss : 0.0000, Prediction: 0.4985\n",
      "Epoch : 8, Loss : 0.0000, Prediction: 0.4987\n",
      "Epoch : 9, Loss : 0.0000, Prediction: 0.4988\n",
      "Epoch : 10, Loss : 0.0000, Prediction: 0.4989\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    loss, y_pred = train_step()\n",
    "    print(f\"Epoch : {epoch +1}, Loss : {loss:.4f}, Prediction: {y_pred[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ffc90-ee9c-4be4-b98e-740bc1926b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
