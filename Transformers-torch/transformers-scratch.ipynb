{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee64c0e2",
   "metadata": {},
   "source": [
    "#### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114687de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a48274",
   "metadata": {},
   "source": [
    "#### Step 2: Multi-Head Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a65a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        print(\"\\n[MultiHeadAttention] Input shape:\", query.shape)\n",
    "\n",
    "        Q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_linear(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_linear(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        print(\"[MultiHeadAttention] Q, K, V shape:\", Q.shape, K.shape, V.shape)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        out = self.out_linear(context)\n",
    "        print(\"[MultiHeadAttention] Output shape:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529433f0",
   "metadata": {},
   "source": [
    "#### Step 3: Positionwise Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9b9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"[FeedForward] Input shape:\", x.shape)\n",
    "        out = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "        print(\"[FeedForward] Output shape:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42c29c",
   "metadata": {},
   "source": [
    "#### Step 4: Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d250c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        print(\"\\n[EncoderLayer] Input shape:\", x.shape)\n",
    "        attn_out = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        print(\"[EncoderLayer] Output shape:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e59d3a",
   "metadata": {},
   "source": [
    "#### Step 5: Transformer Encoder\n",
    "\n",
    "\n",
    "```python\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "```\n",
    "- Initializes the PyTorch module.\n",
    "- `d_model`: the dimensionality of the input and output embeddings.\n",
    "- `num_heads`: number of heads in the multi-head attention.\n",
    "- `d_ff`: hidden layer size in the feedforward network.\n",
    "- `dropout`: for regularization.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "```\n",
    "- Defines a **self-attention block** using multi-head attention.\n",
    "- This will let each token in the input attend to **all other tokens** in the same sequence.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "```\n",
    "- This is a feed-forward network applied **independently to each position** in the sequence.\n",
    "- Helps with non-linear transformation of information after attention.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "```\n",
    "- Layer Normalization layers are used after each sublayer (attention and feedforward).\n",
    "- Help stabilize and speed up training.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "```\n",
    "- Dropout is applied after attention and feedforward layers for regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Pass Logic\n",
    "\n",
    "```python\n",
    "    def forward(self, x, mask=None):\n",
    "        print(\"\\n[EncoderLayer] Input shape:\", x.shape)\n",
    "```\n",
    "- `x`: the input tensor of shape `[batch_size, seq_len, d_model]`.\n",
    "- `mask`: optional mask to avoid attending to padding tokens.\n",
    "- Print helps trace the tensor shape going in.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        attn_out = self.self_attn(x, x, x, mask)\n",
    "```\n",
    "- Applies **self-attention** with `Q = K = V = x`.\n",
    "- Output is a tensor of shape `[batch_size, seq_len, d_model]` where tokens have interacted.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "```\n",
    "- Adds residual connection (`x + attn_out`) and applies dropout and normalization.\n",
    "- Keeps original information and stabilizes training.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        ffn_out = self.ffn(x)\n",
    "```\n",
    "- Passes result through positionwise feedforward network.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        print(\"[EncoderLayer] Output shape:\", x.shape)\n",
    "```\n",
    "- Again, uses a residual connection (`x + ffn_out`), dropout, and layer norm.\n",
    "- Final output shape is printed.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Summary of Data Flow:\n",
    "1. Input ‚Üí Self-Attention ‚Üí Add & Norm ‚Üí Feedforward ‚Üí Add & Norm ‚Üí Output\n",
    "2. Each token becomes **context-aware**, enriched with information from all other tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bae172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        print(\"\\n[Encoder] Input shape:\", x.shape)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"[Encoder] Layer {i+1}\")\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        print(\"[Encoder] Final output shape:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd74b0",
   "metadata": {},
   "source": [
    "#### Step 6: Decoder Layer\n",
    "\n",
    "### 1. **Class Initialization** (`__init__` method):\n",
    "```python\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "```\n",
    "- Initializes the `DecoderLayer` as a subclass of `nn.Module`.\n",
    "- `d_model`, `num_heads`, `d_ff`, and `dropout` are parameters that define the size of the model, the number of attention heads, the feedforward network size, and dropout rate respectively.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Self-Attention Layer** (`self.self_attn`)\n",
    "```python\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "```\n",
    "- `self.self_attn` is a **multi-head self-attention layer** that enables each position in the decoder to attend to all other positions **in the same decoder sequence**.\n",
    "- Input and output for this layer will have shape `[batch_size, seq_len, d_model]`, where `seq_len` is the length of the target sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Cross-Attention Layer** (`self.cross_attn`)\n",
    "```python\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "```\n",
    "- `self.cross_attn` is another **multi-head attention layer**, but it uses the output from the **encoder** as the key and value, while the decoder's current state is the query.\n",
    "- This allows the decoder to focus on relevant parts of the **encoder's output**.\n",
    "- Output shape: `[batch_size, seq_len, d_model]`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Feed-Forward Network** (`self.ffn`)\n",
    "```python\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "```\n",
    "- A **feedforward network** that operates independently on each position (token) in the sequence.\n",
    "- It applies a series of linear transformations and activation functions.\n",
    "- Output shape: `[batch_size, seq_len, d_model]`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Layer Normalization and Dropout Layers**:\n",
    "```python\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "```\n",
    "- These layers are used to apply **layer normalization** and **dropout** to stabilize training and reduce overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Forward Pass** (`forward` method):\n",
    "```python\n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        print(\"\\n[DecoderLayer] Input shape:\", x.shape)\n",
    "```\n",
    "- `x`: the input tensor, typically the decoder's previously generated tokens with shape `[batch_size, seq_len, d_model]`.\n",
    "- `encoder_output`: the output from the **encoder** with shape `[batch_size, seq_len, d_model]`.\n",
    "- `mask`: optional mask to prevent attending to padding tokens.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.1 **Self-Attention**:\n",
    "```python\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, mask)))\n",
    "```\n",
    "- The decoder applies **self-attention** to the current state of the decoder (`x`) by passing it as `Q`, `K`, and `V` to the `self_attn` module.\n",
    "    - **Input** shape of `x`: `[batch_size, seq_len, d_model]`\n",
    "    - **Self-attention output**: `[batch_size, seq_len, d_model]`\n",
    "    - The `mask` (if provided) prevents the decoder from attending to future tokens (important during training).\n",
    "- **Residual Connection**: Adds the original input (`x`) to the self-attention output (`self.attn(x, x, x, mask)`).\n",
    "- **Dropout**: Applied for regularization.\n",
    "- **Layer Normalization**: Applied after the residual connection.\n",
    "    - **Output shape**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.2 **Cross-Attention**:\n",
    "```python\n",
    "        x = self.norm2(x + self.dropout(self.cross_attn(x, encoder_output, encoder_output)))\n",
    "```\n",
    "- Now the decoder attends to the **encoder's output** using **cross-attention**.\n",
    "- The input to the cross-attention layer is the current decoder state (`x`), and the key and value are from the `encoder_output`.\n",
    "    - **Input**: `x` of shape `[batch_size, seq_len, d_model]` (decoder input)\n",
    "    - **Encoder output**: `encoder_output` of shape `[batch_size, seq_len, d_model]`\n",
    "    - The cross-attention output shape: `[batch_size, seq_len, d_model]`\n",
    "- **Residual Connection**: Adds the original `x` to the output of the cross-attention.\n",
    "- **Dropout**: Regularization applied after attention.\n",
    "- **Layer Normalization**: Applied again after the residual connection.\n",
    "    - **Output shape**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.3 **Feed-Forward Network**:\n",
    "```python\n",
    "        x = self.norm3(x + self.dropout(self.ffn(x)))\n",
    "```\n",
    "- The output from the cross-attention block (`x`) is passed through the **positionwise feedforward network**.\n",
    "    - **Feed-forward output shape**: `[batch_size, seq_len, d_model]`\n",
    "- **Residual Connection**: Adds the original `x` to the output of the feed-forward network.\n",
    "- **Dropout**: Applied after the feedforward network.\n",
    "- **Layer Normalization**: Applied after the residual connection.\n",
    "    - **Output shape**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Final Output**:\n",
    "```python\n",
    "        print(\"[DecoderLayer] Output shape:\", x.shape)\n",
    "        return x\n",
    "```\n",
    "- After all three attention and feed-forward steps, the final `x` is returned with the shape `[batch_size, seq_len, d_model]`, which is ready to be passed to the next layer or used in the final output.\n",
    "\n",
    "---\n",
    "\n",
    "### Recap of Matrix Shapes:\n",
    "- **Input `x`**: `[batch_size, seq_len, d_model]` (decoder input)\n",
    "- **Self-Attention output**: `[batch_size, seq_len, d_model]`\n",
    "- **Cross-Attention output**: `[batch_size, seq_len, d_model]`\n",
    "- **Feed-Forward output**: `[batch_size, seq_len, d_model]`\n",
    "- **Final output**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "Each operation, such as self-attention, cross-attention, and feed-forward, preserves the shape `[batch_size, seq_len, d_model]` to ensure the consistency of dimensionality throughout the layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a666ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        print(\"\\n[DecoderLayer] Input shape:\", x.shape)\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, mask)))\n",
    "        x = self.norm2(x + self.dropout(self.cross_attn(x, encoder_output, encoder_output)))\n",
    "        x = self.norm3(x + self.dropout(self.ffn(x)))\n",
    "        print(\"[DecoderLayer] Output shape:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d71e88",
   "metadata": {},
   "source": [
    "#### Step 7: Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f725f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        print(\"\\n[Decoder] Input shape:\", x.shape)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"[Decoder] Layer {i+1}\")\n",
    "            x = layer(x, encoder_output, mask)\n",
    "        x = self.norm(x)\n",
    "        print(\"[Decoder] Final output shape:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216bb3b",
   "metadata": {},
   "source": [
    "### Step 8: Positional Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "353ed79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb1fef",
   "metadata": {},
   "source": [
    "#### Step 9: Full Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "060247be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "        self.decoder = TransformerDecoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, target, mask=None):\n",
    "        print(\"\\n[Model] Starting forward pass\")\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        print(\"[Model] Embedded + Positional Encoded input shape:\", x.shape)\n",
    "\n",
    "        target = self.embedding(target)\n",
    "        target = self.positional_encoding(target)\n",
    "        print(\"[Model] Embedded + Positional Encoded target shape:\", target.shape)\n",
    "        \n",
    "        encoder_output = self.encoder(x, mask)\n",
    "        decoder_output = self.decoder(target, encoder_output, mask)\n",
    "        logits = self.final_linear(decoder_output)\n",
    "        print(\"[Model] Final logits shape:\", logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b801e50",
   "metadata": {},
   "source": [
    "#### Step 10: Tokenization and Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a73a7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "input_sentence = \"My name is Sayantan\"\n",
    "target_sentence = \"Je m'appelle Sayantan\"\n",
    "input_tokens = tokenizer(input_sentence, return_tensors='pt', padding='max_length', truncation=True, max_length=8)\n",
    "target_tokens = tokenizer(target_sentence, return_tensors='pt', padding='max_length', truncation=True, max_length=8)\n",
    "input_ids = input_tokens['input_ids']\n",
    "target_ids = target_tokens['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea4eba",
   "metadata": {},
   "source": [
    "#### Step 10: Run Model and Decode Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27ddee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model] Starting forward pass\n",
      "[Model] Embedded + Positional Encoded input shape: torch.Size([1, 8, 512])\n",
      "[Model] Embedded + Positional Encoded target shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[Encoder] Input shape: torch.Size([1, 8, 512])\n",
      "[Encoder] Layer 1\n",
      "\n",
      "[EncoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[EncoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Encoder] Layer 2\n",
      "\n",
      "[EncoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[EncoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Encoder] Layer 3\n",
      "\n",
      "[EncoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[EncoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Encoder] Layer 4\n",
      "\n",
      "[EncoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[EncoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Encoder] Layer 5\n",
      "\n",
      "[EncoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[EncoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Encoder] Layer 6\n",
      "\n",
      "[EncoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[EncoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Encoder] Final output shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[Decoder] Input shape: torch.Size([1, 8, 512])\n",
      "[Decoder] Layer 1\n",
      "\n",
      "[DecoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[DecoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Decoder] Layer 2\n",
      "\n",
      "[DecoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[DecoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Decoder] Layer 3\n",
      "\n",
      "[DecoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[DecoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Decoder] Layer 4\n",
      "\n",
      "[DecoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[DecoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Decoder] Layer 5\n",
      "\n",
      "[DecoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[DecoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Decoder] Layer 6\n",
      "\n",
      "[DecoderLayer] Input shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "\n",
      "[MultiHeadAttention] Input shape: torch.Size([1, 8, 512])\n",
      "[MultiHeadAttention] Q, K, V shape: torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64]) torch.Size([1, 8, 8, 64])\n",
      "[MultiHeadAttention] Output shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Input shape: torch.Size([1, 8, 512])\n",
      "[FeedForward] Output shape: torch.Size([1, 8, 512])\n",
      "[DecoderLayer] Output shape: torch.Size([1, 8, 512])\n",
      "[Decoder] Final output shape: torch.Size([1, 8, 512])\n",
      "[Model] Final logits shape: torch.Size([1, 8, 30522])\n",
      "\n",
      "[Output Logits Shape]: torch.Size([1, 8, 30522])\n",
      "\n",
      "Predicted Translated Text: galway westwardrize piersvaovao sorority qur\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "model = TransformerModel(num_layers=6, d_model=512, num_heads=8, d_ff=2048, vocab_size=vocab_size)\n",
    "output_logits = model(input_ids, target_ids)\n",
    "print(\"\\n[Output Logits Shape]:\", output_logits.shape)\n",
    "predicted_ids = torch.argmax(output_logits, dim=-1)\n",
    "predicted_tokens = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nPredicted Translated Text:\", predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bce897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
