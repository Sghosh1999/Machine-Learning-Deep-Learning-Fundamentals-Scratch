{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae208435-db2d-41f9-a85a-75a91d1cfbd7",
   "metadata": {},
   "source": [
    "# Simple Neural Network: Manual Backpropagation (1 Hidden Neuron)\n",
    "\n",
    "This tutorial walks through **manual forward and backward pass** in a simple neural network with a single neuron.\n",
    "\n",
    "---\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "- Inputs: `x1, x2 ` \n",
    "- Weights: w11, w12  \n",
    "- Bias: b  \n",
    "- Activation Function: Sigmoid  \n",
    "- Output: y_pred = sigmoid(z)  \n",
    "- Loss: Mean Squared Error (MSE)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Forward Pass\n",
    "\n",
    "Let’s assume the following values:\n",
    "\n",
    "- x1 = 1.0  \n",
    "- x2 = 2.0  \n",
    "- w11 = 0.5  \n",
    "- w12 = -0.4  \n",
    "- b = 0.1  \n",
    "- y (true label) = 1.0\n",
    "\n",
    "### Compute pre-activation (z):\n",
    "z = `w11 * x1 + w12 * x2 + b  `\n",
    "z = 0.5 * 1.0 + (-0.4) * 2.0 + 0.1 = -0.2\n",
    "\n",
    "### Activation using sigmoid:\n",
    "y_pred = `1 / (1 + exp(-z))  `\n",
    "y_pred = 1 / (1 + exp(0.2)) ≈ 0.4502\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Loss Calculation\n",
    "\n",
    "Using Mean Squared Error (MSE):  \n",
    "Loss = `0.5 * (y - y_pred)^2  `\n",
    "Loss = 0.5 * (1.0 - 0.4502)^2  \n",
    "Loss ≈ 0.1511\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Backward Pass (Gradients)\n",
    "\n",
    "### Gradient of loss with respect to output:\n",
    "dL/dy_pred = y_pred - y = 0.4502 - 1.0 = -0.5498\n",
    "\n",
    "### Derivative of sigmoid:\n",
    "dy_pred/dz = y_pred * (1 - y_pred)  \n",
    "           = 0.4502 * (1 - 0.4502) ≈ 0.2475\n",
    "\n",
    "### Gradient of loss with respect to z:\n",
    "dL/dz = dL/dy_pred * dy_pred/dz  \n",
    "      = -0.5498 * 0.2475 ≈ -0.1360\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Gradients w.r.t. Weights and Bias\n",
    "\n",
    "dL/dw11 = dL/dz * x1 = -0.1360 * 1.0 = -0.1360  \n",
    "dL/dw12 = dL/dz * x2 = -0.1360 * 2.0 = -0.2720  \n",
    "dL/db   = dL/dz       = -0.1360\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Weight Update (Gradient Descent)\n",
    "\n",
    "Assuming learning rate (eta) = 0.1\n",
    "\n",
    "w11 = w11 - eta * dL/dw11  \n",
    "    = 0.5 - 0.1 * (-0.1360) = 0.5136\n",
    "\n",
    "w12 = w12 - eta * dL/dw12  \n",
    "    = -0.4 - 0.1 * (-0.2720) = -0.3728\n",
    "\n",
    "b = b - eta * dL/db  \n",
    "  = 0.1 - 0.1 * (-0.1360) = 0.1136\n",
    "\n",
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "- New weights after update:  \n",
    "  - w11 = 0.5136  \n",
    "  - w12 = -0.3728  \n",
    "  - b = 0.1136  \n",
    "\n",
    "- Loss before update ≈ 0.1511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed135a-3a5a-4294-b623-a02eca8d1af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
