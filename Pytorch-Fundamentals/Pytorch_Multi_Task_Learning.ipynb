{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fae01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sayantghosh\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8479238748550415\n",
      "Epoch: 1, Loss: 0.8544884920120239\n",
      "Epoch: 2, Loss: 0.7819072008132935\n",
      "Epoch: 3, Loss: 0.6998901963233948\n",
      "Epoch: 4, Loss: 0.62384432554245\n",
      "Epoch: 5, Loss: 0.5612659454345703\n",
      "Epoch: 6, Loss: 0.4658006429672241\n",
      "Epoch: 7, Loss: 0.2822611629962921\n",
      "Epoch: 8, Loss: 0.29428353905677795\n",
      "Epoch: 9, Loss: 0.2718678116798401\n",
      "Epoch: 10, Loss: 0.2725190222263336\n",
      "Epoch: 11, Loss: 0.24433626234531403\n",
      "Epoch: 12, Loss: 0.21212366223335266\n",
      "Epoch: 13, Loss: 0.18438968062400818\n",
      "Epoch: 14, Loss: 0.15559124946594238\n",
      "Epoch: 15, Loss: 0.12968987226486206\n",
      "Epoch: 16, Loss: 0.11636005342006683\n",
      "Epoch: 17, Loss: 0.09751565754413605\n",
      "Epoch: 18, Loss: 0.0630728080868721\n",
      "Epoch: 19, Loss: 0.08596652001142502\n",
      "Epoch: 20, Loss: 0.1185702234506607\n",
      "Epoch: 21, Loss: 0.034204915165901184\n",
      "Epoch: 22, Loss: 0.04232213273644447\n",
      "Epoch: 23, Loss: 0.030484836548566818\n",
      "Epoch: 24, Loss: 0.01176303718239069\n",
      "Epoch: 25, Loss: 0.04348115250468254\n",
      "Epoch: 26, Loss: 0.058448221534490585\n",
      "Epoch: 27, Loss: 0.005412448663264513\n",
      "Epoch: 28, Loss: 0.06805339455604553\n",
      "Epoch: 29, Loss: 0.06143359839916229\n",
      "Epoch: 30, Loss: 0.022355373948812485\n",
      "Epoch: 31, Loss: 0.043286003172397614\n",
      "Epoch: 32, Loss: 0.047273337841033936\n",
      "Epoch: 33, Loss: 0.0233004130423069\n",
      "Epoch: 34, Loss: 0.0006901794113218784\n",
      "Epoch: 35, Loss: 0.019845522940158844\n",
      "Epoch: 36, Loss: 0.008083796128630638\n",
      "Epoch: 37, Loss: 0.009221019223332405\n",
      "Epoch: 38, Loss: 0.007773487828671932\n",
      "Epoch: 39, Loss: 0.004469904117286205\n",
      "Epoch: 40, Loss: 0.0015394131187349558\n",
      "Epoch: 41, Loss: 0.0021568576339632273\n",
      "Epoch: 42, Loss: 0.002348809503018856\n",
      "Epoch: 43, Loss: 0.002760161180049181\n",
      "Epoch: 44, Loss: 0.003324170596897602\n",
      "Epoch: 45, Loss: 0.020350508391857147\n",
      "Epoch: 46, Loss: 0.006239720620214939\n",
      "Epoch: 47, Loss: 0.01272366289049387\n",
      "Epoch: 48, Loss: 0.003080165246501565\n",
      "Epoch: 49, Loss: 0.007209535222500563\n",
      "Epoch: 50, Loss: 0.0011133227963000536\n",
      "Epoch: 51, Loss: 0.001805442851036787\n",
      "Epoch: 52, Loss: 0.001678568311035633\n",
      "Epoch: 53, Loss: 0.002202180912718177\n",
      "Epoch: 54, Loss: 0.002455435460433364\n",
      "Epoch: 55, Loss: 0.00013148749712854624\n",
      "Epoch: 56, Loss: 0.003931503742933273\n",
      "Epoch: 57, Loss: 0.0037654766347259283\n",
      "Epoch: 58, Loss: 0.00414544390514493\n",
      "Epoch: 59, Loss: 0.0030277706682682037\n",
      "Epoch: 60, Loss: 0.002265740418806672\n",
      "Epoch: 61, Loss: 0.005347027909010649\n",
      "Epoch: 62, Loss: 0.0012191800633445382\n",
      "Epoch: 63, Loss: 0.0011474010534584522\n",
      "Epoch: 64, Loss: 0.0015098436269909143\n",
      "Epoch: 65, Loss: 0.000564312853384763\n",
      "Epoch: 66, Loss: 0.00033445251756347716\n",
      "Epoch: 67, Loss: 0.0004449892439879477\n",
      "Epoch: 68, Loss: 0.0012221095385029912\n",
      "Epoch: 69, Loss: 0.0008016647771000862\n",
      "Epoch: 70, Loss: 0.0006933329859748483\n",
      "Epoch: 71, Loss: 0.0009879074059426785\n",
      "Epoch: 72, Loss: 0.0006241797236725688\n",
      "Epoch: 73, Loss: 0.0006304250564426184\n",
      "Epoch: 74, Loss: 0.013151178136467934\n",
      "Epoch: 75, Loss: 0.0007319636060856283\n",
      "Epoch: 76, Loss: 0.0001915231696330011\n",
      "Epoch: 77, Loss: 0.005789431743323803\n",
      "Epoch: 78, Loss: 0.0005275189178064466\n",
      "Epoch: 79, Loss: 0.0012643453665077686\n",
      "Epoch: 80, Loss: 0.0009246960398741066\n",
      "Epoch: 81, Loss: 0.0016580987721681595\n",
      "Epoch: 82, Loss: 0.002796138171106577\n",
      "Epoch: 83, Loss: 0.011065775528550148\n",
      "Epoch: 84, Loss: 0.0058388602919876575\n",
      "Epoch: 85, Loss: 0.0008736549643799663\n",
      "Epoch: 86, Loss: 0.0005612249951809645\n",
      "Epoch: 87, Loss: 0.0003587139945011586\n",
      "Epoch: 88, Loss: 0.0005150688230060041\n",
      "Epoch: 89, Loss: 0.0008829448488540947\n",
      "Epoch: 90, Loss: 0.0009698896901682019\n",
      "Epoch: 91, Loss: 0.0018949619261547923\n",
      "Epoch: 92, Loss: 0.0014648985816165805\n",
      "Epoch: 93, Loss: 0.0003823729930445552\n",
      "Epoch: 94, Loss: 0.0010465045925229788\n",
      "Epoch: 95, Loss: 0.0006557550514116883\n",
      "Epoch: 96, Loss: 0.002632293850183487\n",
      "Epoch: 97, Loss: 0.001994956284761429\n",
      "Epoch: 98, Loss: 0.00928368978202343\n",
      "Epoch: 99, Loss: 0.0011367411352694035\n",
      "Epoch: 100, Loss: 0.0008081074338406324\n",
      "Epoch: 101, Loss: 0.0015710612060502172\n",
      "Epoch: 102, Loss: 0.0003642453229986131\n",
      "Epoch: 103, Loss: 0.0004650527553167194\n",
      "Epoch: 104, Loss: 0.002813358325511217\n",
      "Epoch: 105, Loss: 0.0016901587368920445\n",
      "Epoch: 106, Loss: 0.001499018631875515\n",
      "Epoch: 107, Loss: 0.001533550675958395\n",
      "Epoch: 108, Loss: 0.004301294684410095\n",
      "Epoch: 109, Loss: 0.0026293345727026463\n",
      "Epoch: 110, Loss: 0.00020570479682646692\n",
      "Epoch: 111, Loss: 0.00024665051023475826\n",
      "Epoch: 112, Loss: 0.00029885012190788984\n",
      "Epoch: 113, Loss: 0.0006505638011731207\n",
      "Epoch: 114, Loss: 0.0003157254250254482\n",
      "Epoch: 115, Loss: 0.002569181378930807\n",
      "Epoch: 116, Loss: 0.004615746904164553\n",
      "Epoch: 117, Loss: 0.0005034393980167806\n",
      "Epoch: 118, Loss: 0.0011398602509871125\n",
      "Epoch: 119, Loss: 0.0012137370649725199\n",
      "Epoch: 120, Loss: 0.0018757422221824527\n",
      "Epoch: 121, Loss: 0.0007394756539724767\n",
      "Epoch: 122, Loss: 0.002089613815769553\n",
      "Epoch: 123, Loss: 0.0014495003269985318\n",
      "Epoch: 124, Loss: 0.0010633270721882582\n",
      "Epoch: 125, Loss: 4.523097231867723e-05\n",
      "Epoch: 126, Loss: 0.0003984934592153877\n",
      "Epoch: 127, Loss: 0.00045357964700087905\n",
      "Epoch: 128, Loss: 0.0002497792011126876\n",
      "Epoch: 129, Loss: 0.00021319488587323576\n",
      "Epoch: 130, Loss: 0.0015014326199889183\n",
      "Epoch: 131, Loss: 0.00011557537800399587\n",
      "Epoch: 132, Loss: 0.00012263616372365505\n",
      "Epoch: 133, Loss: 0.0001440099731553346\n",
      "Epoch: 134, Loss: 0.00023907066497486085\n",
      "Epoch: 135, Loss: 0.00016769947251304984\n",
      "Epoch: 136, Loss: 0.005946767516434193\n",
      "Epoch: 137, Loss: 0.00024003026192076504\n",
      "Epoch: 138, Loss: 0.0021166675724089146\n",
      "Epoch: 139, Loss: 0.00015062696184031665\n",
      "Epoch: 140, Loss: 0.0002860349486581981\n",
      "Epoch: 141, Loss: 0.0002484788419678807\n",
      "Epoch: 142, Loss: 0.000683542457409203\n",
      "Epoch: 143, Loss: 7.250021735671908e-05\n",
      "Epoch: 144, Loss: 0.00013905995001550764\n",
      "Epoch: 145, Loss: 0.00015148433158174157\n",
      "Epoch: 146, Loss: 0.000266392802586779\n",
      "Epoch: 147, Loss: 0.00011926059232791886\n",
      "Epoch: 148, Loss: 0.0001225478044943884\n",
      "Epoch: 149, Loss: 0.00017919877427630126\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "class MTLNetwork(nn.Module):  # Inherit from nn.Module\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(MTLNetwork, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Cosine Task\n",
    "        self.model_cosine = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Sine Task\n",
    "        self.sin_cosine = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x1 = self.model(x)\n",
    "        \n",
    "        # Generate Cosine predictions\n",
    "        cosine_output = self.model_cosine(x1)\n",
    "        \n",
    "        # Generate Sine predictions\n",
    "        sine_output = self.sin_cosine(x1)\n",
    "        \n",
    "        return cosine_output, sine_output\n",
    "\n",
    "# Instantiate the model\n",
    "model = MTLNetwork(hidden_size=128, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for i in range(150):\n",
    "    optimizer.zero_grad()\n",
    "    x = torch.randn(1, 1)\n",
    "    cosine_output, sine_output = model(x)\n",
    "    \n",
    "    # Calculate losses for both tasks\n",
    "    # Assuming cosine task is a classification task and sine task is a regression task\n",
    "    cosine_target = torch.tensor([0])  # Target class for cosine task\n",
    "    sine_target = torch.tensor([0.5])  # Target value for sine task\n",
    "\n",
    "    cosine_loss = CrossEntropyLoss()(cosine_output, cosine_target)\n",
    "    sine_loss = MSELoss()(sine_output, sine_target)\n",
    "    \n",
    "    loss = cosine_loss + sine_loss\n",
    "    print(f\"Epoch: {i}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec8d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1786]])\n",
      "Cosine Output: tensor([[ 5.3871, -5.3975]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.randn(1, 1)\n",
    "\n",
    "print(y)\n",
    "# Test the model\n",
    "y_pred = model(y)\n",
    "print(f\"Cosine Output: {y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c89f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ac4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
